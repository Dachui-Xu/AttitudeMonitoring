{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 随机森林",
   "id": "5b76c46a82a585c6"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-08T00:47:33.575758Z",
     "start_time": "2024-07-08T00:47:28.250633Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import glob\n",
    "\n",
    "def extract_features_from_window(window):\n",
    "    features = []\n",
    "    for column in window.columns:\n",
    "        sensor_data = window[column]\n",
    "        \n",
    "        # Detect positive pulses\n",
    "        pulses = sensor_data[sensor_data > sensor_data.mean()]\n",
    "        \n",
    "        # Extract features: height, duration, area, max, min, mean, std\n",
    "        heights = pulses.values\n",
    "        durations = np.diff(pulses.index)\n",
    "        \n",
    "        # Adjust lengths to match\n",
    "        heights = heights[:-1]  # Drop the last height to match durations length\n",
    "        \n",
    "        areas = heights * durations\n",
    "        max_height = np.max(heights) if len(heights) > 0 else 0\n",
    "        min_height = np.min(heights) if len(heights) > 0 else 0\n",
    "        mean_height = np.mean(heights) if len(heights) > 0 else 0\n",
    "        std_height = np.std(heights) if len(heights) > 0 else 0\n",
    "        pulse_frequency = len(heights) / len(sensor_data)\n",
    "        \n",
    "        features.extend([max_height, min_height, mean_height, std_height, pulse_frequency])\n",
    "    \n",
    "    return features\n",
    "\n",
    "def prepare_dataset_from_windows(df, state, window_size=89):\n",
    "    X = []\n",
    "    y = []\n",
    "    for start in range(0, len(df) - window_size + 1, window_size):\n",
    "        window = df.iloc[start:start + window_size]\n",
    "        features = extract_features_from_window(window)\n",
    "        X.append(features)\n",
    "        y.append(state)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def load_and_process_files(file_paths, window_size=89):\n",
    "    all_X = []\n",
    "    all_y = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        # Extract state from the file name\n",
    "        state = file_path.split('/')[-1].split('_')[0]\n",
    "        print(f\"Processing file: {file_path} with state: {state}\")\n",
    "        \n",
    "        # Load and process the CSV file\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        df.columns = [f's{i}' for i in range(len(df.columns))]\n",
    "        \n",
    "        # Prepare dataset from windows\n",
    "        X, y = prepare_dataset_from_windows(df, state, window_size)\n",
    "        \n",
    "        all_X.append(X)\n",
    "        all_y.append(y)\n",
    "    \n",
    "    if not all_X:\n",
    "        print(\"No data to process.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Combine data from all files\n",
    "    all_X = np.vstack(all_X)\n",
    "    all_y = np.concatenate(all_y)\n",
    "    \n",
    "    return all_X, all_y\n",
    "\n",
    "def main():\n",
    "    file_paths = glob.glob('*.csv')  # Adjust the path as necessary\n",
    "    window_size = 89  # Set the window size to 89 or 90 based on your preference\n",
    "\n",
    "    # Load and process files\n",
    "    all_X, all_y = load_and_process_files(file_paths, window_size)\n",
    "    \n",
    "    if all_X is None or all_y is None:\n",
    "        print(\"No data to process.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Feature extraction and dataset preparation completed.\")\n",
    "    print(f\"Total samples: {len(all_y)}\")\n",
    "    \n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    all_X = scaler.fit_transform(all_X)\n",
    "    \n",
    "    # Split dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(all_X, all_y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'max_depth': [10, 20, 30, None],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    }\n",
    "    \n",
    "    # Initialize the Random Forest classifier\n",
    "    clf = RandomForestClassifier()\n",
    "    \n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "    \n",
    "    # Fit the model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Best model\n",
    "    best_clf = grid_search.best_estimator_\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_pred = best_clf.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Save the model\n",
    "    joblib.dump(best_clf, 'best_random_forest_model.pkl')\n",
    "    print(\"Model saved as 'best_random_forest_model.pkl'\")\n",
    "\n",
    "# Run the main function\n",
    "main()\n"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 交叉验证和模型保存",
   "id": "f02bbac82fc134cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T00:48:41.605879Z",
     "start_time": "2024-07-08T00:48:35.800841Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import glob\n",
    "\n",
    "def extract_features_from_window(window):\n",
    "    features = []\n",
    "    for column in window.columns:\n",
    "        sensor_data = window[column]\n",
    "        \n",
    "        # Detect positive pulses\n",
    "        pulses = sensor_data[sensor_data > sensor_data.mean()]\n",
    "        \n",
    "        # Extract features: height, duration, area, max, min, mean, std\n",
    "        heights = pulses.values\n",
    "        durations = np.diff(pulses.index)\n",
    "        \n",
    "        # Adjust lengths to match\n",
    "        heights = heights[:-1]  # Drop the last height to match durations length\n",
    "        \n",
    "        areas = heights * durations\n",
    "        max_height = np.max(heights) if len(heights) > 0 else 0\n",
    "        min_height = np.min(heights) if len(heights) > 0 else 0\n",
    "        mean_height = np.mean(heights) if len(heights) > 0 else 0\n",
    "        std_height = np.std(heights) if len(heights) > 0 else 0\n",
    "        pulse_frequency = len(heights) / len(sensor_data)\n",
    "        \n",
    "        features.extend([max_height, min_height, mean_height, std_height, pulse_frequency])\n",
    "    \n",
    "    return features\n",
    "\n",
    "def prepare_dataset_from_windows(df, state, window_size=89):\n",
    "    X = []\n",
    "    y = []\n",
    "    for start in range(0, len(df) - window_size + 1, window_size):\n",
    "        window = df.iloc[start:start + window_size]\n",
    "        features = extract_features_from_window(window)\n",
    "        X.append(features)\n",
    "        y.append(state)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def load_and_process_files(file_paths, window_size=89):\n",
    "    all_X = []\n",
    "    all_y = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        # Extract state from the file name\n",
    "        state = file_path.split('/')[-1].split('_')[0]\n",
    "        print(f\"Processing file: {file_path} with state: {state}\")\n",
    "        \n",
    "        # Load and process the CSV file\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        df.columns = [f's{i}' for i in range(len(df.columns))]\n",
    "        \n",
    "        # Prepare dataset from windows\n",
    "        X, y = prepare_dataset_from_windows(df, state, window_size)\n",
    "        \n",
    "        all_X.append(X)\n",
    "        all_y.append(y)\n",
    "    \n",
    "    if not all_X:\n",
    "        print(\"No data to process.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Combine data from all files\n",
    "    all_X = np.vstack(all_X)\n",
    "    all_y = np.concatenate(all_y)\n",
    "    \n",
    "    return all_X, all_y\n",
    "\n",
    "def main():\n",
    "    file_paths = glob.glob('*.csv')  # Adjust the path as necessary\n",
    "    window_size = 89  # Set the window size to 89 or 90 based on your preference\n",
    "\n",
    "    # Load and process files\n",
    "    all_X, all_y = load_and_process_files(file_paths, window_size)\n",
    "    \n",
    "    if all_X is None or all_y is None:\n",
    "        print(\"No data to process.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Feature extraction and dataset preparation completed.\")\n",
    "    print(f\"Total samples: {len(all_y)}\")\n",
    "    \n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    all_X = scaler.fit_transform(all_X)\n",
    "    \n",
    "    # Define the parameter grid\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'max_depth': [10, 20, 30, None],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    }\n",
    "    \n",
    "    # Initialize the Random Forest classifier\n",
    "    clf = RandomForestClassifier()\n",
    "    \n",
    "    # Initialize GridSearchCV\n",
    "    grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "    \n",
    "    # Fit the model using GridSearchCV\n",
    "    grid_search.fit(all_X, all_y)\n",
    "    \n",
    "    # Best model\n",
    "    best_clf = grid_search.best_estimator_\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(best_clf, all_X, all_y, cv=5)\n",
    "    print(f\"Cross-validation scores: {cv_scores}\")\n",
    "    print(f\"Mean cross-validation score: {np.mean(cv_scores)}\")\n",
    "    \n",
    "    # Split dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(all_X, all_y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Fit the best model on the training set\n",
    "    best_clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict and evaluate\n",
    "    y_pred = best_clf.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Save the model\n",
    "    joblib.dump(best_clf, 'best_random_forest_model.pkl')\n",
    "    print(\"Model saved as 'best_random_forest_model.pkl'\")\n",
    "\n",
    "# Run the main function\n",
    "main()\n"
   ],
   "id": "8173e7f1281506bb",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T00:49:44.857316Z",
     "start_time": "2024-07-08T00:49:26.043156Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import glob\n",
    "\n",
    "def extract_features_from_window(window):\n",
    "    features = []\n",
    "    for column in window.columns:\n",
    "        sensor_data = window[column]\n",
    "        \n",
    "        # Detect positive pulses\n",
    "        pulses = sensor_data[sensor_data > sensor_data.mean()]\n",
    "        \n",
    "        # Extract features: height, duration, area, max, min, mean, std\n",
    "        heights = pulses.values\n",
    "        durations = np.diff(pulses.index)\n",
    "        \n",
    "        # Adjust lengths to match\n",
    "        heights = heights[:-1]  # Drop the last height to match durations length\n",
    "        \n",
    "        areas = heights * durations\n",
    "        max_height = np.max(heights) if len(heights) > 0 else 0\n",
    "        min_height = np.min(heights) if len(heights) > 0 else 0\n",
    "        mean_height = np.mean(heights) if len(heights) > 0 else 0\n",
    "        std_height = np.std(heights) if len(heights) > 0 else 0\n",
    "        pulse_frequency = len(heights) / len(sensor_data)\n",
    "        \n",
    "        # Additional features\n",
    "        energy = np.sum(sensor_data ** 2)  # Signal energy\n",
    "        mean = np.mean(sensor_data)  # Mean value\n",
    "        std = np.std(sensor_data)  # Standard deviation\n",
    "        var = np.var(sensor_data)  # Variance\n",
    "        rms = np.sqrt(np.mean(sensor_data ** 2))  # Root mean square\n",
    "        \n",
    "        features.extend([\n",
    "            max_height, min_height, mean_height, std_height, pulse_frequency,\n",
    "            energy, mean, std, var, rms\n",
    "        ])\n",
    "    \n",
    "    return features\n",
    "\n",
    "def prepare_dataset_from_windows(df, state, window_size=89):\n",
    "    X = []\n",
    "    y = []\n",
    "    for start in range(0, len(df) - window_size + 1, window_size):\n",
    "        window = df.iloc[start:start + window_size]\n",
    "        features = extract_features_from_window(window)\n",
    "        X.append(features)\n",
    "        y.append(state)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def load_and_process_files(file_paths, window_size=89):\n",
    "    data = {}\n",
    "    for file_path in file_paths:\n",
    "        # Extract state from the file name\n",
    "        state = file_path.split('/')[-1].split('_')[0]\n",
    "        print(f\"Processing file: {file_path} with state: {state}\")\n",
    "        \n",
    "        # Load and process the CSV file\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        df.columns = [f's{i}' for i in range(len(df.columns))]\n",
    "        \n",
    "        # Prepare dataset from windows\n",
    "        X, y = prepare_dataset_from_windows(df, state, window_size)\n",
    "        \n",
    "        data[state] = (X, y)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def leave_one_out_cross_validation(data):\n",
    "    all_states = list(data.keys())\n",
    "    results = {}\n",
    "\n",
    "    for state in all_states:\n",
    "        # Use the current state as the validation set\n",
    "        X_val, y_val = data[state]\n",
    "        \n",
    "        # Use the other states as the training set\n",
    "        X_train = np.vstack([data[s][0] for s in all_states if s != state])\n",
    "        y_train = np.concatenate([data[s][1] for s in all_states if s != state])\n",
    "        \n",
    "        # Standardize the features\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "        \n",
    "        # Define the parameter grid\n",
    "        param_grid = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_features': ['sqrt', 'log2'],\n",
    "            'max_depth': [10, 20, 30, None],\n",
    "            'criterion': ['gini', 'entropy']\n",
    "        }\n",
    "        \n",
    "        # Initialize the Random Forest classifier\n",
    "        clf = RandomForestClassifier()\n",
    "        \n",
    "        # Initialize GridSearchCV\n",
    "        grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "        \n",
    "        # Fit the model using GridSearchCV\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Best model\n",
    "        best_clf = grid_search.best_estimator_\n",
    "        \n",
    "        # Predict and evaluate\n",
    "        y_pred = best_clf.predict(X_val)\n",
    "        report = classification_report(y_val, y_pred, output_dict=True, zero_division=0)\n",
    "        results[state] = report\n",
    "        print(f\"Validation on state {state}:\")\n",
    "        print(classification_report(y_val, y_pred, zero_division=0))\n",
    "        \n",
    "        # Print distribution of predictions\n",
    "        unique, counts = np.unique(y_pred, return_counts=True)\n",
    "        prediction_distribution = dict(zip(unique, counts))\n",
    "        print(f\"Prediction distribution for state {state}: {prediction_distribution}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    file_paths = glob.glob('*.csv')  # Adjust the path as necessary\n",
    "    window_size = 89  # Set the window size to 89 or 90 based on your preference\n",
    "\n",
    "    # Load and process files\n",
    "    data = load_and_process_files(file_paths, window_size)\n",
    "    \n",
    "    if not data:\n",
    "        print(\"No data to process.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Feature extraction and dataset preparation completed.\")\n",
    "    \n",
    "    # Perform leave-one-out cross-validation\n",
    "    results = leave_one_out_cross_validation(data)\n",
    "    \n",
    "    # Print the results\n",
    "    for state, report in results.items():\n",
    "        print(f\"Results for state {state}:\")\n",
    "        print(report)\n",
    "\n",
    "# Run the main function\n",
    "main()\n"
   ],
   "id": "e25dc687cd4f10cd",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 支持向量机",
   "id": "22440b4c5f3b3417"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T00:49:52.425884Z",
     "start_time": "2024-07-08T00:49:46.321409Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import glob\n",
    "\n",
    "def extract_features_from_window(window):\n",
    "    features = []\n",
    "    for column in window.columns:\n",
    "        sensor_data = window[column]\n",
    "        \n",
    "        # Detect positive pulses\n",
    "        pulses = sensor_data[sensor_data > sensor_data.mean()]\n",
    "        \n",
    "        # Extract features: height, duration, area, max, min, mean, std\n",
    "        heights = pulses.values\n",
    "        durations = np.diff(pulses.index)\n",
    "        \n",
    "        # Adjust lengths to match\n",
    "        heights = heights[:-1]  # Drop the last height to match durations length\n",
    "        \n",
    "        areas = heights * durations\n",
    "        max_height = np.max(heights) if len(heights) > 0 else 0\n",
    "        min_height = np.min(heights) if len(heights) > 0 else 0\n",
    "        mean_height = np.mean(heights) if len(heights) > 0 else 0\n",
    "        std_height = np.std(heights) if len(heights) > 0 else 0\n",
    "        pulse_frequency = len(heights) / len(sensor_data)\n",
    "        \n",
    "        # Additional features\n",
    "        energy = np.sum(sensor_data ** 2)  # Signal energy\n",
    "        mean = np.mean(sensor_data)  # Mean value\n",
    "        std = np.std(sensor_data)  # Standard deviation\n",
    "        var = np.var(sensor_data)  # Variance\n",
    "        rms = np.sqrt(np.mean(sensor_data ** 2))  # Root mean square\n",
    "        skewness = pd.Series(sensor_data).skew()  # Skewness\n",
    "        kurtosis = pd.Series(sensor_data).kurt()  # Kurtosis\n",
    "        \n",
    "        features.extend([\n",
    "            max_height, min_height, mean_height, std_height, pulse_frequency,\n",
    "            energy, mean, std, var, rms, skewness, kurtosis\n",
    "        ])\n",
    "    \n",
    "    return features\n",
    "\n",
    "def prepare_dataset_from_windows(df, state, window_size=89):\n",
    "    X = []\n",
    "    y = []\n",
    "    for start in range(0, len(df) - window_size + 1, window_size):\n",
    "        window = df.iloc[start:start + window_size]\n",
    "        features = extract_features_from_window(window)\n",
    "        X.append(features)\n",
    "        y.append(state)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def load_and_process_files(file_paths, window_size=89):\n",
    "    all_X = []\n",
    "    all_y = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        # Extract state from the file name\n",
    "        state = file_path.split('/')[-1].split('_')[0]\n",
    "        print(f\"Processing file: {file_path} with state: {state}\")\n",
    "        \n",
    "        # Load and process the CSV file\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        df.columns = [f's{i}' for i in range(len(df.columns))]\n",
    "        \n",
    "        # Prepare dataset from windows\n",
    "        X, y = prepare_dataset_from_windows(df, state, window_size)\n",
    "        \n",
    "        all_X.append(X)\n",
    "        all_y.append(y)\n",
    "    \n",
    "    if not all_X:\n",
    "        print(\"No data to process.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Combine data from all files\n",
    "    all_X = np.vstack(all_X)\n",
    "    all_y = np.concatenate(all_y)\n",
    "    \n",
    "    return all_X, all_y\n",
    "\n",
    "def train_and_evaluate_model(X, y):\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    # Split dataset into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Define the parameter grid for RandomForest\n",
    "    param_grid_rf = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'max_depth': [10, 20, 30, None],\n",
    "        'criterion': ['gini', 'entropy']\n",
    "    }\n",
    "    \n",
    "    # Define the parameter grid for SVM\n",
    "    param_grid_svm = {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    }\n",
    "    \n",
    "    # Initialize the RandomForest classifier\n",
    "    clf_rf = RandomForestClassifier()\n",
    "    \n",
    "    # Initialize the SVM classifier\n",
    "    clf_svm = SVC()\n",
    "    \n",
    "    # Initialize GridSearchCV for RandomForest\n",
    "    grid_search_rf = GridSearchCV(estimator=clf_rf, param_grid=param_grid_rf, cv=5, n_jobs=-1, verbose=2)\n",
    "    \n",
    "    # Initialize GridSearchCV for SVM\n",
    "    grid_search_svm = GridSearchCV(estimator=clf_svm, param_grid=param_grid_svm, cv=5, n_jobs=-1, verbose=2)\n",
    "    \n",
    "    # Fit the model using GridSearchCV for RandomForest\n",
    "    grid_search_rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Fit the model using GridSearchCV for SVM\n",
    "    grid_search_svm.fit(X_train, y_train)\n",
    "    \n",
    "    # Best models\n",
    "    best_clf_rf = grid_search_rf.best_estimator_\n",
    "    best_clf_svm = grid_search_svm.best_estimator_\n",
    "    \n",
    "    # Predict and evaluate RandomForest\n",
    "    y_pred_rf = best_clf_rf.predict(X_test)\n",
    "    print(\"RandomForest Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred_rf, zero_division=0))\n",
    "    \n",
    "    # Predict and evaluate SVM\n",
    "    y_pred_svm = best_clf_svm.predict(X_test)\n",
    "    print(\"SVM Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred_svm, zero_division=0))\n",
    "    \n",
    "    # Save the models\n",
    "    joblib.dump(best_clf_rf, 'best_random_forest_model.pkl')\n",
    "    print(\"RandomForest model saved as 'best_random_forest_model.pkl'\")\n",
    "    \n",
    "    joblib.dump(best_clf_svm, 'best_svm_model.pkl')\n",
    "    print(\"SVM model saved as 'best_svm_model.pkl'\")\n",
    "\n",
    "def main():\n",
    "    file_paths = glob.glob('*.csv')  # Adjust the path as necessary\n",
    "    window_size = 89  # Set the window size to 89 or 90 based on your preference\n",
    "\n",
    "    # Load and process files\n",
    "    all_X, all_y = load_and_process_files(file_paths, window_size)\n",
    "    \n",
    "    if all_X is None or all_y is None:\n",
    "        print(\"No data to process.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Feature extraction and dataset preparation completed.\")\n",
    "    print(f\"Total samples: {len(all_y)}\")\n",
    "    \n",
    "    # Train and evaluate model\n",
    "    train_and_evaluate_model(all_X, all_y)\n",
    "\n",
    "# Run the main function\n",
    "main()\n"
   ],
   "id": "bd6d5cfda44de541",
   "execution_count": 8,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T00:50:10.268739Z",
     "start_time": "2024-07-08T00:50:09.271412Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import glob\n",
    "\n",
    "def extract_features_from_window(window):\n",
    "    features = []\n",
    "    for column in window.columns:\n",
    "        sensor_data = window[column]\n",
    "        \n",
    "        # Detect positive pulses\n",
    "        pulses = sensor_data[sensor_data > sensor_data.mean()]\n",
    "        \n",
    "        # Extract features: height, duration, area, max, min, mean, std\n",
    "        heights = pulses.values\n",
    "        durations = np.diff(pulses.index)\n",
    "        \n",
    "        # Adjust lengths to match\n",
    "        heights = heights[:-1]  # Drop the last height to match durations length\n",
    "        \n",
    "        areas = heights * durations\n",
    "        max_height = np.max(heights) if len(heights) > 0 else 0\n",
    "        min_height = np.min(heights) if len(heights) > 0 else 0\n",
    "        mean_height = np.mean(heights) if len(heights) > 0 else 0\n",
    "        std_height = np.std(heights) if len(heights) > 0 else 0\n",
    "        pulse_frequency = len(heights) / len(sensor_data)\n",
    "        \n",
    "        # Additional features\n",
    "        energy = np.sum(sensor_data ** 2)  # Signal energy\n",
    "        mean = np.mean(sensor_data)  # Mean value\n",
    "        std = np.std(sensor_data)  # Standard deviation\n",
    "        var = np.var(sensor_data)  # Variance\n",
    "        rms = np.sqrt(np.mean(sensor_data ** 2))  # Root mean square\n",
    "        skewness = pd.Series(sensor_data).skew()  # Skewness\n",
    "        kurtosis = pd.Series(sensor_data).kurt()  # Kurtosis\n",
    "        \n",
    "        features.extend([\n",
    "            max_height, min_height, mean_height, std_height, pulse_frequency,\n",
    "            energy, mean, std, var, rms, skewness, kurtosis\n",
    "        ])\n",
    "    \n",
    "    return features\n",
    "\n",
    "def prepare_dataset_from_windows(df, state, window_size=89):\n",
    "    X = []\n",
    "    y = []\n",
    "    for start in range(0, len(df) - window_size + 1, window_size):\n",
    "        window = df.iloc[start:start + window_size]\n",
    "        features = extract_features_from_window(window)\n",
    "        X.append(features)\n",
    "        y.append(state)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def load_and_process_files(file_paths, window_size=89):\n",
    "    data = {}\n",
    "    for file_path in file_paths:\n",
    "        # Extract state from the file name\n",
    "        state = file_path.split('/')[-1].split('_')[0]\n",
    "        print(f\"Processing file: {file_path} with state: {state}\")\n",
    "        \n",
    "        # Load and process the CSV file\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        df.columns = [f's{i}' for i in range(len(df.columns))]\n",
    "        \n",
    "        # Prepare dataset from windows\n",
    "        X, y = prepare_dataset_from_windows(df, state, window_size)\n",
    "        \n",
    "        data[state] = (X, y)\n",
    "    \n",
    "    return data\n",
    "\n",
    "def leave_one_out_cross_validation(data):\n",
    "    all_states = list(data.keys())\n",
    "    results = {}\n",
    "\n",
    "    for state in all_states:\n",
    "        # Use the current state as the validation set\n",
    "        X_val, y_val = data[state]\n",
    "        \n",
    "        # Use the other states as the training set\n",
    "        X_train = np.vstack([data[s][0] for s in all_states if s != state])\n",
    "        y_train = np.concatenate([data[s][1] for s in all_states if s != state])\n",
    "        \n",
    "        # Standardize the features\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "        \n",
    "        # Define the parameter grid for SVM\n",
    "        param_grid = {\n",
    "            'C': [0.1, 1, 10, 100],\n",
    "            'gamma': ['scale', 'auto'],\n",
    "            'kernel': ['linear', 'rbf']\n",
    "        }\n",
    "        \n",
    "        # Initialize the SVM classifier\n",
    "        clf = SVC()\n",
    "        \n",
    "        # Initialize GridSearchCV\n",
    "        grid_search = GridSearchCV(estimator=clf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)\n",
    "        \n",
    "        # Fit the model using GridSearchCV\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Best model\n",
    "        best_clf = grid_search.best_estimator_\n",
    "        \n",
    "        # Cross-validation\n",
    "        cv_scores = cross_val_score(best_clf, X_train, y_train, cv=5)\n",
    "        print(f\"Cross-validation scores for state {state}: {cv_scores}\")\n",
    "        print(f\"Mean cross-validation score for state {state}: {np.mean(cv_scores)}\")\n",
    "        \n",
    "        # Predict and evaluate\n",
    "        y_pred = best_clf.predict(X_val)\n",
    "        report = classification_report(y_val, y_pred, output_dict=True, zero_division=0)\n",
    "        results[state] = report\n",
    "        print(f\"Validation on state {state}:\")\n",
    "        print(classification_report(y_val, y_pred, zero_division=0))\n",
    "        \n",
    "        # Print distribution of predictions\n",
    "        unique, counts = np.unique(y_pred, return_counts=True)\n",
    "        prediction_distribution = dict(zip(unique, counts))\n",
    "        print(f\"Prediction distribution for state {state}: {prediction_distribution}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    file_paths = glob.glob('*.csv')  # Adjust the path as necessary\n",
    "    window_size = 89  # Set the window size to 89 or 90 based on your preference\n",
    "\n",
    "    # Load and process files\n",
    "    data = load_and_process_files(file_paths, window_size)\n",
    "    \n",
    "    if not data:\n",
    "        print(\"No data to process.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Feature extraction and dataset preparation completed.\")\n",
    "    \n",
    "    # Perform leave-one-out cross-validation\n",
    "    results = leave_one_out_cross_validation(data)\n",
    "    \n",
    "    # Print the results\n",
    "    for state, report in results.items():\n",
    "        print(f\"Results for state {state}:\")\n",
    "        print(report)\n",
    "\n",
    "# Run the main function\n",
    "main()\n"
   ],
   "id": "200a1f680f4b3249",
   "execution_count": 9,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T00:50:20.649792Z",
     "start_time": "2024-07-08T00:50:19.900262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "import glob\n",
    "\n",
    "def extract_features_from_window(window):\n",
    "    features = []\n",
    "    for column in window.columns:\n",
    "        sensor_data = window[column]\n",
    "        \n",
    "        # Detect positive pulses\n",
    "        pulses = sensor_data[sensor_data > sensor_data.mean()]\n",
    "        \n",
    "        # Extract features: height, duration, area, max, min, mean, std\n",
    "        heights = pulses.values\n",
    "        durations = np.diff(pulses.index)\n",
    "        \n",
    "        # Adjust lengths to match\n",
    "        heights = heights[:-1]  # Drop the last height to match durations length\n",
    "        \n",
    "        areas = heights * durations\n",
    "        max_height = np.max(heights) if len(heights) > 0 else 0\n",
    "        min_height = np.min(heights) if len(heights) > 0 else 0\n",
    "        mean_height = np.mean(heights) if len(heights) > 0 else 0\n",
    "        std_height = np.std(heights) if len(heights) > 0 else 0\n",
    "        pulse_frequency = len(heights) / len(sensor_data)\n",
    "        \n",
    "        # Additional features\n",
    "        energy = np.sum(sensor_data ** 2)  # Signal energy\n",
    "        mean = np.mean(sensor_data)  # Mean value\n",
    "        std = np.std(sensor_data)  # Standard deviation\n",
    "        var = np.var(sensor_data)  # Variance\n",
    "        rms = np.sqrt(np.mean(sensor_data ** 2))  # Root mean square\n",
    "        skewness = pd.Series(sensor_data).skew()  # Skewness\n",
    "        kurtosis = pd.Series(sensor_data).kurt()  # Kurtosis\n",
    "        \n",
    "        features.extend([\n",
    "            max_height, min_height, mean_height, std_height, pulse_frequency,\n",
    "            energy, mean, std, var, rms, skewness, kurtosis\n",
    "        ])\n",
    "    \n",
    "    return features\n",
    "\n",
    "def prepare_dataset_from_windows(df, window_size=89):\n",
    "    X = []\n",
    "    for start in range(0, len(df) - window_size + 1, window_size):\n",
    "        window = df.iloc[start:start + window_size]\n",
    "        features = extract_features_from_window(window)\n",
    "        X.append(features)\n",
    "    return np.array(X)\n",
    "\n",
    "def load_and_process_files(file_paths, window_size=89):\n",
    "    all_X = []\n",
    "    all_y = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        # Extract state from the file name\n",
    "        state = file_path.split('/')[-1].split('_')[0]\n",
    "        print(f\"Processing file: {file_path} with state: {state}\")\n",
    "        \n",
    "        # Load and process the CSV file\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        df.columns = [f's{i}' for i in range(len(df.columns))]\n",
    "        \n",
    "        # Prepare dataset from windows\n",
    "        X = prepare_dataset_from_windows(df, window_size)\n",
    "        \n",
    "        all_X.append(X)\n",
    "        all_y.extend([state] * len(X))\n",
    "    \n",
    "    if not all_X:\n",
    "        print(\"No data to process.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Combine data from all files\n",
    "    all_X = np.vstack(all_X)\n",
    "    all_y = np.array(all_y)\n",
    "    \n",
    "    return all_X, all_y\n",
    "\n",
    "def main():\n",
    "    # 加载已经保存的模型\n",
    "    model = joblib.load('best_svm_model.pkl')\n",
    "    print(\"Model loaded from 'best_svm_model.pkl'\")\n",
    "    \n",
    "    # 加载和处理新数据\n",
    "    file_paths = glob.glob('*.csv')  # Adjust the path as necessary\n",
    "    window_size = 89  # Set the window size to 89 or 90 based on your preference\n",
    "    \n",
    "    all_X, all_y = load_and_process_files(file_paths, window_size)\n",
    "    \n",
    "    if all_X is None or all_y is None:\n",
    "        print(\"No data to process.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Feature extraction and dataset preparation completed.\")\n",
    "    print(f\"Total samples: {len(all_y)}\")\n",
    "    \n",
    "    # 标准化特征\n",
    "    scaler = StandardScaler()\n",
    "    all_X = scaler.fit_transform(all_X)\n",
    "    \n",
    "    # 使用加载的模型进行预测\n",
    "    y_pred = model.predict(all_X)\n",
    "    \n",
    "    # 评估模型性能\n",
    "    report = classification_report(all_y, y_pred, output_dict=True, zero_division=0)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(all_y, y_pred, zero_division=0))\n",
    "    \n",
    "    # 打印预测结果的分布\n",
    "    unique, counts = np.unique(y_pred, return_counts=True)\n",
    "    prediction_distribution = dict(zip(unique, counts))\n",
    "    print(f\"Prediction distribution: {prediction_distribution}\")\n",
    "\n",
    "# 运行主函数\n",
    "main()\n"
   ],
   "id": "289d3f2a57023a8e",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T00:50:34.839693Z",
     "start_time": "2024-07-08T00:50:34.093032Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "import glob\n",
    "\n",
    "def extract_features_from_window(window):\n",
    "    features = []\n",
    "    for column in window.columns:\n",
    "        sensor_data = window[column]\n",
    "        \n",
    "        # Detect positive pulses\n",
    "        pulses = sensor_data[sensor_data > sensor_data.mean()]\n",
    "        \n",
    "        # Extract features: height, duration, area, max, min, mean, std\n",
    "        heights = pulses.values\n",
    "        durations = np.diff(pulses.index)\n",
    "        \n",
    "        # Adjust lengths to match\n",
    "        heights = heights[:-1]  # Drop the last height to match durations length\n",
    "        \n",
    "        areas = heights * durations\n",
    "        max_height = np.max(heights) if len(heights) > 0 else 0\n",
    "        min_height = np.min(heights) if len(heights) > 0 else 0\n",
    "        mean_height = np.mean(heights) if len(heights) > 0 else 0\n",
    "        std_height = np.std(heights) if len(heights) > 0 else 0\n",
    "        pulse_frequency = len(heights) / len(sensor_data)\n",
    "        \n",
    "        # Additional features\n",
    "        energy = np.sum(sensor_data ** 2)  # Signal energy\n",
    "        mean = np.mean(sensor_data)  # Mean value\n",
    "        std = np.std(sensor_data)  # Standard deviation\n",
    "        var = np.var(sensor_data)  # Variance\n",
    "        rms = np.sqrt(np.mean(sensor_data ** 2))  # Root mean square\n",
    "        skewness = pd.Series(sensor_data).skew()  # Skewness\n",
    "        kurtosis = pd.Series(sensor_data).kurt()  # Kurtosis\n",
    "        \n",
    "        features.extend([\n",
    "            max_height, min_height, mean_height, std_height, pulse_frequency,\n",
    "            energy, mean, std, var, rms, skewness, kurtosis\n",
    "        ])\n",
    "    \n",
    "    return features\n",
    "\n",
    "def prepare_dataset_from_windows(df, window_size=89):\n",
    "    X = []\n",
    "    for start in range(0, len(df) - window_size + 1, window_size):\n",
    "        window = df.iloc[start:start + window_size]\n",
    "        features = extract_features_from_window(window)\n",
    "        X.append(features)\n",
    "    return np.array(X)\n",
    "\n",
    "def load_and_process_files(file_paths, window_size=89):\n",
    "    data = {}\n",
    "    for file_path in file_paths:\n",
    "        # Extract state from the file name\n",
    "        state = file_path.split('/')[-1].split('_')[0]\n",
    "        print(f\"Processing file: {file_path} with state: {state}\")\n",
    "        \n",
    "        # Load and process the CSV file\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        df.columns = [f's{i}' for i in range(len(df.columns))]\n",
    "        \n",
    "        # Prepare dataset from windows\n",
    "        X = prepare_dataset_from_windows(df, window_size)\n",
    "        \n",
    "        data[state] = (X, [state] * len(X))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def leave_one_out_cross_validation_with_loaded_model(data, model_path):\n",
    "    all_states = list(data.keys())\n",
    "    results = {}\n",
    "\n",
    "    # Load the pre-trained model\n",
    "    model = joblib.load(model_path)\n",
    "    print(f\"Model loaded from '{model_path}'\")\n",
    "\n",
    "    for state in all_states:\n",
    "        # Use the current state as the validation set\n",
    "        X_val, y_val = data[state]\n",
    "        \n",
    "        # Use the other states as the training set\n",
    "        X_train = np.vstack([data[s][0] for s in all_states if s != state])\n",
    "        y_train = np.concatenate([data[s][1] for s in all_states if s != state])\n",
    "        \n",
    "        # Standardize the features\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "        \n",
    "        # Evaluate the loaded model on the validation set\n",
    "        y_pred = model.predict(X_val)\n",
    "        report = classification_report(y_val, y_pred, output_dict=True, zero_division=0)\n",
    "        results[state] = report\n",
    "        print(f\"Validation on state {state}:\")\n",
    "        print(classification_report(y_val, y_pred, zero_division=0))\n",
    "        \n",
    "        # Print distribution of predictions\n",
    "        unique, counts = np.unique(y_pred, return_counts=True)\n",
    "        prediction_distribution = dict(zip(unique, counts))\n",
    "        print(f\"Prediction distribution for state {state}: {prediction_distribution}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    file_paths = glob.glob('*.csv')  # Adjust the path as necessary\n",
    "    window_size = 89  # Set the window size to 89 or 90 based on your preference\n",
    "\n",
    "    # Load and process files\n",
    "    data = load_and_process_files(file_paths, window_size)\n",
    "    \n",
    "    if not data:\n",
    "        print(\"No data to process.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Feature extraction and dataset preparation completed.\")\n",
    "    \n",
    "    # Perform leave-one-out cross-validation with the loaded model\n",
    "    model_path = 'best_svm_model.pkl'  # Path to the saved model\n",
    "    results = leave_one_out_cross_validation_with_loaded_model(data, model_path)\n",
    "    \n",
    "    # Print the results\n",
    "    for state, report in results.items():\n",
    "        print(f\"Results for state {state}:\")\n",
    "        print(report)\n",
    "\n",
    "# Run the main function\n",
    "main()\n"
   ],
   "id": "fe4c82a35bfd14d9",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T00:50:45.127762Z",
     "start_time": "2024-07-08T00:50:44.381720Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "import glob\n",
    "\n",
    "def extract_features_from_window(window):\n",
    "    features = []\n",
    "    for column in window.columns:\n",
    "        sensor_data = window[column]\n",
    "        \n",
    "        # Detect positive pulses\n",
    "        pulses = sensor_data[sensor_data > sensor_data.mean()]\n",
    "        \n",
    "        # Extract features: height, duration, area, max, min, mean, std\n",
    "        heights = pulses.values\n",
    "        durations = np.diff(pulses.index)\n",
    "        \n",
    "        # Adjust lengths to match\n",
    "        heights = heights[:-1]  # Drop the last height to match durations length\n",
    "        \n",
    "        areas = heights * durations\n",
    "        max_height = np.max(heights) if len(heights) > 0 else 0\n",
    "        min_height = np.min(heights) if len(heights) > 0 else 0\n",
    "        mean_height = np.mean(heights) if len(heights) > 0 else 0\n",
    "        std_height = np.std(heights) if len(heights) > 0 else 0\n",
    "        pulse_frequency = len(heights) / len(sensor_data)\n",
    "        \n",
    "        # Additional features\n",
    "        energy = np.sum(sensor_data ** 2)  # Signal energy\n",
    "        mean = np.mean(sensor_data)  # Mean value\n",
    "        std = np.std(sensor_data)  # Standard deviation\n",
    "        var = np.var(sensor_data)  # Variance\n",
    "        rms = np.sqrt(np.mean(sensor_data ** 2))  # Root mean square\n",
    "        skewness = pd.Series(sensor_data).skew()  # Skewness\n",
    "        kurtosis = pd.Series(sensor_data).kurt()  # Kurtosis\n",
    "        \n",
    "        features.extend([\n",
    "            max_height, min_height, mean_height, std_height, pulse_frequency,\n",
    "            energy, mean, std, var, rms, skewness, kurtosis\n",
    "        ])\n",
    "    \n",
    "    return features\n",
    "\n",
    "def prepare_dataset_from_windows(df, window_size=89):\n",
    "    X = []\n",
    "    for start in range(0, len(df) - window_size + 1, window_size):\n",
    "        window = df.iloc[start:start + window_size]\n",
    "        features = extract_features_from_window(window)\n",
    "        X.append(features)\n",
    "    return np.array(X)\n",
    "\n",
    "def load_and_process_files(file_paths, window_size=89):\n",
    "    data = {}\n",
    "    for file_path in file_paths:\n",
    "        # Extract state from the file name\n",
    "        state = file_path.split('/')[-1].split('_')[0]\n",
    "        print(f\"Processing file: {file_path} with state: {state}\")\n",
    "        \n",
    "        # Load and process the CSV file\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        df.columns = [f's{i}' for i in range(len(df.columns))]\n",
    "        \n",
    "        # Prepare dataset from windows\n",
    "        X = prepare_dataset_from_windows(df, window_size)\n",
    "        \n",
    "        data[state] = (X, [state] * len(X))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def leave_one_out_cross_validation_with_loaded_model(data, model_path):\n",
    "    all_states = list(data.keys())\n",
    "    results = {}\n",
    "\n",
    "    # Load the pre-trained model\n",
    "    model = joblib.load(model_path)\n",
    "    print(f\"Model loaded from '{model_path}'\")\n",
    "\n",
    "    for state in all_states:\n",
    "        # Use the current state as the validation set\n",
    "        X_val, y_val = data[state]\n",
    "        \n",
    "        # Use the other states as the training set\n",
    "        X_train = np.vstack([data[s][0] for s in all_states if s != state])\n",
    "        y_train = np.concatenate([data[s][1] for s in all_states if s != state])\n",
    "        \n",
    "        # Standardize the features\n",
    "        scaler = StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_val = scaler.transform(X_val)\n",
    "        \n",
    "        # Evaluate the loaded model on the validation set\n",
    "        y_pred = model.predict(X_val)\n",
    "        report = classification_report(y_val, y_pred, output_dict=True, zero_division=0)\n",
    "        results[state] = report\n",
    "        print(f\"Validation on state {state}:\")\n",
    "        print(classification_report(y_val, y_pred, zero_division=0))\n",
    "        \n",
    "        # Print distribution of predictions\n",
    "        unique, counts = np.unique(y_pred, return_counts=True)\n",
    "        prediction_distribution = dict(zip(unique, counts))\n",
    "        print(f\"Prediction distribution for state {state}: {prediction_distribution}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    file_paths = glob.glob('*.csv')  # Adjust the path as necessary\n",
    "    window_size = 89  # Set the window size to 89 or 90 based on your preference\n",
    "\n",
    "    # Load and process files\n",
    "    data = load_and_process_files(file_paths, window_size)\n",
    "    \n",
    "    if not data:\n",
    "        print(\"No data to process.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Feature extraction and dataset preparation completed.\")\n",
    "    \n",
    "    # Perform leave-one-out cross-validation with the loaded model\n",
    "    model_path = 'best_svm_model.pkl'  # Path to the saved model\n",
    "    results = leave_one_out_cross_validation_with_loaded_model(data, model_path)\n",
    "    \n",
    "    # Print the results\n",
    "    for state, report in results.items():\n",
    "        print(f\"Results for state {state}:\")\n",
    "        print(report)\n",
    "\n",
    "# Run the main function\n",
    "main()\n"
   ],
   "id": "865013c3266692e6",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## SVM交叉验证",
   "id": "c61e5564e6746dd1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T05:28:42.109952Z",
     "start_time": "2024-07-08T05:28:41.150719Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "import glob\n",
    "\n",
    "def extract_features_from_window(window):\n",
    "    features = []\n",
    "    for column in window.columns:\n",
    "        sensor_data = window[column]\n",
    "        \n",
    "        # Detect positive pulses\n",
    "        pulses = sensor_data[sensor_data > sensor_data.mean()]\n",
    "        \n",
    "        # Extract features: height, duration, area, max, min, mean, std\n",
    "        heights = pulses.values\n",
    "        durations = np.diff(pulses.index)\n",
    "        \n",
    "        # Adjust lengths to match\n",
    "        heights = heights[:-1]  # Drop the last height to match durations length\n",
    "        \n",
    "        areas = heights * durations\n",
    "        max_height = np.max(heights) if len(heights) > 0 else 0\n",
    "        min_height = np.min(heights) if len(heights) > 0 else 0\n",
    "        mean_height = np.mean(heights) if len(heights) > 0 else 0\n",
    "        std_height = np.std(heights) if len(heights) > 0 else 0\n",
    "        pulse_frequency = len(heights) / len(sensor_data)\n",
    "        \n",
    "        # Additional features\n",
    "        energy = np.sum(sensor_data ** 2)  # Signal energy\n",
    "        mean = np.mean(sensor_data)  # Mean value\n",
    "        std = np.std(sensor_data)  # Standard deviation\n",
    "        var = np.var(sensor_data)  # Variance\n",
    "        rms = np.sqrt(np.mean(sensor_data ** 2))  # Root mean square\n",
    "        skewness = pd.Series(sensor_data).skew()  # Skewness\n",
    "        kurtosis = pd.Series(sensor_data).kurt()  # Kurtosis\n",
    "        \n",
    "        features.extend([\n",
    "            max_height, min_height, mean_height, std_height, pulse_frequency,\n",
    "            energy, mean, std, var, rms, skewness, kurtosis\n",
    "        ])\n",
    "    \n",
    "    return features\n",
    "\n",
    "def prepare_dataset_from_windows(df, window_size=89):\n",
    "    X = []\n",
    "    for start in range(0, len(df) - window_size + 1, window_size):\n",
    "        window = df.iloc[start:start + window_size]\n",
    "        features = extract_features_from_window(window)\n",
    "        X.append(features)\n",
    "    return np.array(X)\n",
    "\n",
    "def load_and_process_files(file_paths, window_size=89):\n",
    "    all_X = []\n",
    "    all_y = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        # Extract state from the file name\n",
    "        state = file_path.split('/')[-1].split('_')[0]\n",
    "        print(f\"Processing file: {file_path} with state: {state}\")\n",
    "        \n",
    "        # Load and process the CSV file\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        df.columns = [f's{i}' for i in range(len(df.columns))]\n",
    "        \n",
    "        # Prepare dataset from windows\n",
    "        X = prepare_dataset_from_windows(df, window_size)\n",
    "        \n",
    "        all_X.append(X)\n",
    "        all_y.extend([state] * len(X))\n",
    "    \n",
    "    if not all_X:\n",
    "        print(\"No data to process.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Combine data from all files\n",
    "    all_X = np.vstack(all_X)\n",
    "    all_y = np.array(all_y)\n",
    "    \n",
    "    return all_X, all_y\n",
    "\n",
    "def main():\n",
    "    # 加载已经保存的模型\n",
    "    model = joblib.load('best_svm_model.pkl')\n",
    "    print(\"Model loaded from 'best_svm_model.pkl'\")\n",
    "    \n",
    "    # 加载和处理新数据\n",
    "    file_paths = glob.glob('*.csv')  # Adjust the path as necessary\n",
    "    window_size = 89  # Set the window size to 89 or 90 based on your preference\n",
    "    \n",
    "    all_X, all_y = load_and_process_files(file_paths, window_size)\n",
    "    \n",
    "    if all_X is None or all_y is None:\n",
    "        print(\"No data to process.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Feature extraction and dataset preparation completed.\")\n",
    "    print(f\"Total samples: {len(all_y)}\")\n",
    "    \n",
    "    # 标准化特征\n",
    "    scaler = StandardScaler()\n",
    "    all_X = scaler.fit_transform(all_X)\n",
    "    \n",
    "    # 使用交叉验证评估模型性能\n",
    "    cv_scores = cross_val_score(model, all_X, all_y, cv=5)\n",
    "    print(f\"Cross-validation scores: {cv_scores}\")\n",
    "    print(f\"Mean cross-validation score: {np.mean(cv_scores)}\")\n",
    "    \n",
    "    # 使用加载的模型进行预测\n",
    "    y_pred = model.predict(all_X)\n",
    "    \n",
    "    # 评估模型性能\n",
    "    report = classification_report(all_y, y_pred, output_dict=True, zero_division=0)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(all_y, y_pred, zero_division=0))\n",
    "    \n",
    "    # 打印预测结果的分布\n",
    "    unique, counts = np.unique(y_pred, return_counts=True)\n",
    "    prediction_distribution = dict(zip(unique, counts))\n",
    "    print(f\"Prediction distribution: {prediction_distribution}\")\n",
    "\n",
    "# 运行主函数\n",
    "main()\n"
   ],
   "id": "594b4cdb9c27a251",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 随机森林交叉验证",
   "id": "4beec3d349641fc2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T05:34:48.120609Z",
     "start_time": "2024-07-08T05:34:47.326195Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "import glob\n",
    "\n",
    "def extract_features_from_window(window):\n",
    "    features = []\n",
    "    for column in window.columns:\n",
    "        sensor_data = window[column]\n",
    "        \n",
    "        # Detect positive pulses\n",
    "        pulses = sensor_data[sensor_data > sensor_data.mean()]\n",
    "        \n",
    "        # Extract features: height, duration, area, max, min, mean, std\n",
    "        heights = pulses.values\n",
    "        durations = np.diff(pulses.index)\n",
    "        \n",
    "        # Adjust lengths to match\n",
    "        heights = heights[:-1]  # Drop the last height to match durations length\n",
    "        \n",
    "        areas = heights * durations\n",
    "        max_height = np.max(heights) if len(heights) > 0 else 0\n",
    "        min_height = np.min(heights) if len(heights) > 0 else 0\n",
    "        mean_height = np.mean(heights) if len(heights) > 0 else 0\n",
    "        std_height = np.std(heights) if len(heights) > 0 else 0\n",
    "        pulse_frequency = len(heights) / len(sensor_data)\n",
    "        \n",
    "        # Additional features\n",
    "        energy = np.sum(sensor_data ** 2)  # Signal energy\n",
    "        mean = np.mean(sensor_data)  # Mean value\n",
    "        std = np.std(sensor_data)  # Standard deviation\n",
    "        var = np.var(sensor_data)  # Variance\n",
    "        rms = np.sqrt(np.mean(sensor_data ** 2))  # Root mean square\n",
    "        skewness = pd.Series(sensor_data).skew()  # Skewness\n",
    "        kurtosis = pd.Series(sensor_data).kurt()  # Kurtosis\n",
    "        \n",
    "        features.extend([\n",
    "            max_height, min_height, mean_height, std_height, pulse_frequency,\n",
    "            energy, mean, std, var, rms, skewness, kurtosis\n",
    "        ])\n",
    "    \n",
    "    return features\n",
    "\n",
    "def prepare_dataset_from_windows(df, window_size=89):\n",
    "    X = []\n",
    "    for start in range(0, len(df) - window_size + 1, window_size):\n",
    "        window = df.iloc[start:start + window_size]\n",
    "        features = extract_features_from_window(window)\n",
    "        X.append(features)\n",
    "    return np.array(X)\n",
    "\n",
    "def load_and_process_files(file_paths, window_size=89):\n",
    "    all_X = []\n",
    "    all_y = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        # Extract state from the file name\n",
    "        state = file_path.split('/')[-1].split('_')[0]\n",
    "        print(f\"Processing file: {file_path} with state: {state}\")\n",
    "        \n",
    "        # Load and process the CSV file\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        df.columns = [f's{i}' for i in range(len(df.columns))]\n",
    "        \n",
    "        # Prepare dataset from windows\n",
    "        X = prepare_dataset_from_windows(df, window_size)\n",
    "        \n",
    "        all_X.append(X)\n",
    "        all_y.extend([state] * len(X))\n",
    "    \n",
    "    if not all_X:\n",
    "        print(\"No data to process.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Combine data from all files\n",
    "    all_X = np.vstack(all_X)\n",
    "    all_y = np.array(all_y)\n",
    "    \n",
    "    return all_X, all_y\n",
    "\n",
    "def main():\n",
    "    # 加载已经保存的模型\n",
    "    model = joblib.load('best_svm_model.pkl')\n",
    "    print(\"Model loaded from 'best_random_forest_model.pkl'\")\n",
    "    \n",
    "    # 加载和处理新数据\n",
    "    file_paths = glob.glob('*.csv')  # Adjust the path as necessary\n",
    "    window_size = 89  # Set the window size to 89 or 90 based on your preference\n",
    "    \n",
    "    all_X, all_y = load_and_process_files(file_paths, window_size)\n",
    "    \n",
    "    if all_X is None or all_y is None:\n",
    "        print(\"No data to process.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Feature extraction and dataset preparation completed.\")\n",
    "    print(f\"Total samples: {len(all_y)}\")\n",
    "    \n",
    "    # 标准化特征\n",
    "    scaler = StandardScaler()\n",
    "    all_X = scaler.fit_transform(all_X)\n",
    "    \n",
    "    # 使用交叉验证评估模型性能\n",
    "    cv_scores = cross_val_score(model, all_X, all_y, cv=5)\n",
    "    print(f\"Cross-validation scores: {cv_scores}\")\n",
    "    print(f\"Mean cross-validation score: {np.mean(cv_scores)}\")\n",
    "    \n",
    "    # 使用加载的模型进行预测\n",
    "    y_pred = model.predict(all_X)\n",
    "    \n",
    "    # 评估模型性能\n",
    "    report = classification_report(all_y, y_pred, output_dict=True, zero_division=0)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(all_y, y_pred, zero_division=0))\n",
    "    \n",
    "    # 打印预测结果的分布\n",
    "    unique, counts = np.unique(y_pred, return_counts=True)\n",
    "    prediction_distribution = dict(zip(unique, counts))\n",
    "    print(f\"Prediction distribution: {prediction_distribution}\")\n",
    "\n",
    "# 运行主函数\n",
    "main()\n"
   ],
   "id": "25f5c62e3226b5ee",
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## SVM和随机森林一起验证",
   "id": "b297d4b951dc1a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T05:33:57.308599Z",
     "start_time": "2024-07-08T05:33:54.502201Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import joblib\n",
    "import glob\n",
    "\n",
    "def extract_features_from_window(window):\n",
    "    features = []\n",
    "    for column in window.columns:\n",
    "        sensor_data = window[column]\n",
    "        \n",
    "        # Detect positive pulses\n",
    "        pulses = sensor_data[sensor_data > sensor_data.mean()]\n",
    "        \n",
    "        # Extract features: height, duration, area, max, min, mean, std\n",
    "        heights = pulses.values\n",
    "        durations = np.diff(pulses.index)\n",
    "        \n",
    "        # Adjust lengths to match\n",
    "        heights = heights[:-1]  # Drop the last height to match durations length\n",
    "        \n",
    "        areas = heights * durations\n",
    "        max_height = np.max(heights) if len(heights) > 0 else 0\n",
    "        min_height = np.min(heights) if len(heights) > 0 else 0\n",
    "        mean_height = np.mean(heights) if len(heights) > 0 else 0\n",
    "        std_height = np.std(heights) if len(heights) > 0 else 0\n",
    "        pulse_frequency = len(heights) / len(sensor_data)\n",
    "        \n",
    "        # Additional features\n",
    "        energy = np.sum(sensor_data ** 2)  # Signal energy\n",
    "        mean = np.mean(sensor_data)  # Mean value\n",
    "        std = np.std(sensor_data)  # Standard deviation\n",
    "        var = np.var(sensor_data)  # Variance\n",
    "        rms = np.sqrt(np.mean(sensor_data ** 2))  # Root mean square\n",
    "        skewness = pd.Series(sensor_data).skew()  # Skewness\n",
    "        kurtosis = pd.Series(sensor_data).kurt()  # Kurtosis\n",
    "        \n",
    "        features.extend([\n",
    "            max_height, min_height, mean_height, std_height, pulse_frequency,\n",
    "            energy, mean, std, var, rms, skewness, kurtosis\n",
    "        ])\n",
    "    \n",
    "    return features\n",
    "\n",
    "def prepare_dataset_from_windows(df, window_size=89):\n",
    "    X = []\n",
    "    for start in range(0, len(df) - window_size + 1, window_size):\n",
    "        window = df.iloc[start:start + window_size]\n",
    "        features = extract_features_from_window(window)\n",
    "        X.append(features)\n",
    "    return np.array(X)\n",
    "\n",
    "def load_and_process_files(file_paths, window_size=89):\n",
    "    all_X = []\n",
    "    all_y = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        # Extract state from the file name\n",
    "        state = file_path.split('/')[-1].split('_')[0]\n",
    "        print(f\"Processing file: {file_path} with state: {state}\")\n",
    "        \n",
    "        # Load and process the CSV file\n",
    "        try:\n",
    "            df = pd.read_csv(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        df.columns = [f's{i}' for i in range(len(df.columns))]\n",
    "        \n",
    "        # Prepare dataset from windows\n",
    "        X = prepare_dataset_from_windows(df, window_size)\n",
    "        \n",
    "        all_X.append(X)\n",
    "        all_y.extend([state] * len(X))\n",
    "    \n",
    "    if not all_X:\n",
    "        print(\"No data to process.\")\n",
    "        return None, None\n",
    "    \n",
    "    # Combine data from all files\n",
    "    all_X = np.vstack(all_X)\n",
    "    all_y = np.array(all_y)\n",
    "    \n",
    "    return all_X, all_y\n",
    "\n",
    "def main():\n",
    "    # 加载模型\n",
    "    rf_model = joblib.load('best_random_forest_model.pkl')\n",
    "    svm_model = joblib.load('best_svm_model.pkl')\n",
    "    print(\"Models loaded from 'best_random_forest_model.pkl' and 'best_svm_model.pkl'\")\n",
    "    \n",
    "    # 加载和处理数据\n",
    "    file_paths = glob.glob('*.csv')  # Adjust the path as necessary\n",
    "    window_size = 89  # Set the window size to 89 or 90 based on your preference\n",
    "    \n",
    "    all_X, all_y = load_and_process_files(file_paths, window_size)\n",
    "    \n",
    "    if all_X is None or all_y is None:\n",
    "        print(\"No data to process.\")\n",
    "        return\n",
    "    \n",
    "    print(\"Feature extraction and dataset preparation completed.\")\n",
    "    print(f\"Total samples: {len(all_y)}\")\n",
    "    \n",
    "    # 标准化特征\n",
    "    scaler = StandardScaler()\n",
    "    all_X = scaler.fit_transform(all_X)\n",
    "    \n",
    "    # 交叉验证评估随机森林模型性能\n",
    "    rf_cv_scores = cross_val_score(rf_model, all_X, all_y, cv=5)\n",
    "    print(f\"Random Forest Cross-validation scores: {rf_cv_scores}\")\n",
    "    print(f\"Random Forest Mean cross-validation score: {np.mean(rf_cv_scores)}\")\n",
    "    \n",
    "    # 交叉验证评估SVM模型性能\n",
    "    svm_cv_scores = cross_val_score(svm_model, all_X, all_y, cv=5)\n",
    "    print(f\"SVM Cross-validation scores: {svm_cv_scores}\")\n",
    "    print(f\"SVM Mean cross-validation score: {np.mean(svm_cv_scores)}\")\n",
    "    \n",
    "    # 使用随机森林模型进行预测\n",
    "    y_pred_rf = rf_model.predict(all_X)\n",
    "    \n",
    "    # 使用SVM模型进行预测\n",
    "    y_pred_svm = svm_model.predict(all_X)\n",
    "    \n",
    "    # 评估随机森林模型性能\n",
    "    rf_report = classification_report(all_y, y_pred_rf, output_dict=True, zero_division=0)\n",
    "    print(\"Random Forest Classification Report:\")\n",
    "    print(classification_report(all_y, y_pred_rf, zero_division=0))\n",
    "    \n",
    "    # 评估SVM模型性能\n",
    "    svm_report = classification_report(all_y, y_pred_svm, output_dict=True, zero_division=0)\n",
    "    print(\"SVM Classification Report:\")\n",
    "    print(classification_report(all_y, y_pred_svm, zero_division=0))\n",
    "    \n",
    "    # 打印预测结果的分布\n",
    "    unique_rf, counts_rf = np.unique(y_pred_rf, return_counts=True)\n",
    "    prediction_distribution_rf = dict(zip(unique_rf, counts_rf))\n",
    "    print(f\"Random Forest Prediction distribution: {prediction_distribution_rf}\")\n",
    "    \n",
    "    unique_svm, counts_svm = np.unique(y_pred_svm, return_counts=True)\n",
    "    prediction_distribution_svm = dict(zip(unique_svm, counts_svm))\n",
    "    print(f\"SVM Prediction distribution: {prediction_distribution_svm}\")\n",
    "\n",
    "# 运行主函数\n",
    "main()\n"
   ],
   "id": "f4d7e06c6b59bc07",
   "execution_count": 17,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "当然，我可以详细解释分类报告中的各个参数，以及它们的意义。\n",
    "\n",
    "### 分类报告参数解释\n",
    "\n",
    "分类报告是评估分类模型性能的工具，主要包括以下几个指标：\n",
    "\n",
    "1. **Precision（精度）**：\n",
    "   - 定义：精度是正确预测的正样本数占所有预测为正样本的样本数的比例。\n",
    "   - 计算公式：`Precision = TP / (TP + FP)`，其中TP为真阳性，FP为假阳性。\n",
    "   - 意义：精度反映了模型在预测正样本时的准确性，即模型预测为正的样本中有多少是正确的。\n",
    "\n",
    "2. **Recall（召回率）**：\n",
    "   - 定义：召回率是正确预测的正样本数占所有实际为正样本的样本数的比例。\n",
    "   - 计算公式：`Recall = TP / (TP + FN)`，其中FN为假阴性。\n",
    "   - 意义：召回率反映了模型对正样本的识别能力，即模型能够识别出多少实际为正的样本。\n",
    "\n",
    "3. **F1-Score（F1分数）**：\n",
    "   - 定义：F1分数是精度和召回率的调和平均数。\n",
    "   - 计算公式：`F1-Score = 2 * (Precision * Recall) / (Precision + Recall)`\n",
    "   - 意义：F1分数综合了精度和召回率，是模型在精度和召回率之间的一种平衡。\n",
    "\n",
    "4. **Support（支持度）**：\n",
    "   - 定义：支持度是每个类别在测试集中出现的次数。\n",
    "   - 意义：支持度反映了每个类别在测试集中的样本数量。\n",
    "\n",
    "5. **Accuracy（准确率）**：\n",
    "   - 定义：准确率是正确预测的样本数占所有样本数的比例。\n",
    "   - 计算公式：`Accuracy = (TP + TN) / (TP + TN + FP + FN)`\n",
    "   - 意义：准确率反映了模型总体的正确率。\n",
    "\n",
    "6. **Macro Avg（宏平均）**：\n",
    "   - 定义：宏平均是所有类别精度、召回率和F1分数的算术平均值。\n",
    "   - 意义：宏平均对每个类别给予相同的权重，不考虑类别的不平衡。\n",
    "\n",
    "7. **Weighted Avg（加权平均）**：\n",
    "   - 定义：加权平均是所有类别精度、召回率和F1分数的加权平均值，权重为每个类别的支持度。\n",
    "   - 意义：加权平均考虑了类别的不平衡，更能反映整体性能。\n",
    "\n",
    "### 示例分类报告\n",
    "\n",
    "```plaintext\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "        down       1.00      0.99      0.99       100\n",
    "        left       0.98      1.00      0.99       100\n",
    "       right       1.00      0.99      0.99       100\n",
    "          up       1.00      1.00      1.00       100\n",
    "\n",
    "    accuracy                           0.99       400\n",
    "   macro avg       1.00      0.99      1.00       400\n",
    "weighted avg       1.00      0.99      1.00       400\n",
    "```\n",
    "\n",
    "### 解读示例分类报告\n",
    "\n",
    "1. **down** 类别：\n",
    "   - 精度：1.00\n",
    "     - 模型预测为“down”的样本中，100%是正确的。\n",
    "   - 召回率：0.99\n",
    "     - 实际为“down”的样本中，99%被模型正确识别。\n",
    "   - F1分数：0.99\n",
    "     - 精度和召回率的调和平均数为0.99。\n",
    "   - 支持度：100\n",
    "     - 测试集中“down”样本的数量为100。\n",
    "\n",
    "2. **left** 类别：\n",
    "   - 精度：0.98\n",
    "     - 模型预测为“left”的样本中，98%是正确的。\n",
    "   - 召回率：1.00\n",
    "     - 实际为“left”的样本中，100%被模型正确识别。\n",
    "   - F1分数：0.99\n",
    "     - 精度和召回率的调和平均数为0.99。\n",
    "   - 支持度：100\n",
    "     - 测试集中“left”样本的数量为100。\n",
    "\n",
    "3. **overall accuracy**：\n",
    "   - 准确率：0.99\n",
    "     - 总体上，模型在400个样本中有99%的预测是正确的。\n",
    "\n",
    "4. **macro avg**：\n",
    "   - 宏平均：\n",
    "     - 精度：1.00\n",
    "     - 召回率：0.99\n",
    "     - F1分数：1.00\n",
    "   - 宏平均值反映了所有类别的总体表现。\n",
    "\n",
    "5. **weighted avg**：\n",
    "   - 加权平均：\n",
    "     - 精度：1.00\n",
    "     - 召回率：0.99\n",
    "     - F1分数：1.00\n",
    "   - 加权平均值反映了考虑类别不平衡后的总体表现。\n",
    "\n",
    "这些指标可以帮助你全面了解模型的性能，识别模型在特定类别上的强项和弱项，并做出相应的调整和改进。如果有任何问题或需要进一步的解释，请告诉我！"
   ],
   "id": "47589b1ddd5ba908"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 制作相似数据，测试SVM",
   "id": "503cfa5589e14ecc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T06:05:19.181596Z",
     "start_time": "2024-07-08T06:05:19.157041Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def analyze_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    stats = df.describe().transpose()\n",
    "    return stats[['mean', 'std']]\n",
    "\n",
    "def generate_synthetic_data(stats, num_samples=89):\n",
    "    synthetic_data = {}\n",
    "    for column in stats.index:\n",
    "        mean = stats.loc[column, 'mean']\n",
    "        std = stats.loc[column, 'std']\n",
    "        synthetic_data[column] = np.random.normal(mean, std, num_samples)\n",
    "    return pd.DataFrame(synthetic_data)\n",
    "\n",
    "def extract_features_from_window(window):\n",
    "    features = []\n",
    "    for column in window.columns:\n",
    "        sensor_data = window[column]\n",
    "        \n",
    "        # Detect positive pulses\n",
    "        pulses = sensor_data[sensor_data > sensor_data.mean()]\n",
    "        \n",
    "        # Extract features: height, duration, area, max, min, mean, std\n",
    "        heights = pulses.values\n",
    "        durations = np.diff(pulses.index)\n",
    "        \n",
    "        # Adjust lengths to match\n",
    "        heights = heights[:-1]  # Drop the last height to match durations length\n",
    "        \n",
    "        areas = heights * durations\n",
    "        max_height = np.max(heights) if len(heights) > 0 else 0\n",
    "        min_height = np.min(heights) if len(heights) > 0 else 0\n",
    "        mean_height = np.mean(heights) if len(heights) > 0 else 0\n",
    "        std_height = np.std(heights) if len(heights) > 0 else 0\n",
    "        pulse_frequency = len(heights) / len(sensor_data)\n",
    "        \n",
    "        # Additional features\n",
    "        energy = np.sum(sensor_data ** 2)  # Signal energy\n",
    "        mean = np.mean(sensor_data)  # Mean value\n",
    "        std = np.std(sensor_data)  # Standard deviation\n",
    "        var = np.var(sensor_data)  # Variance\n",
    "        rms = np.sqrt(np.mean(sensor_data ** 2))  # Root mean square\n",
    "        skewness = pd.Series(sensor_data).skew()  # Skewness\n",
    "        kurtosis = pd.Series(sensor_data).kurt()  # Kurtosis\n",
    "        \n",
    "        features.extend([\n",
    "            max_height, min_height, mean_height, std_height, pulse_frequency,\n",
    "            energy, mean, std, var, rms, skewness, kurtosis\n",
    "        ])\n",
    "    \n",
    "    return features\n",
    "\n",
    "def main():\n",
    "    # 读取和分析提供的数据文件\n",
    "    file_path = 'left_processed.csv'\n",
    "    stats = analyze_data(file_path)\n",
    "    \n",
    "    # 生成与提供数据相似的新数据\n",
    "    synthetic_data = generate_synthetic_data(stats)\n",
    "    print(\"Generated synthetic data:\")\n",
    "    print(synthetic_data)\n",
    "    \n",
    "    # 加载模型\n",
    "    model = joblib.load('best_svm_model.pkl')\n",
    "    print(\"Model loaded from 'best_svm_model.pkl'\")\n",
    "    \n",
    "    # 提取特征\n",
    "    features = extract_features_from_window(synthetic_data)\n",
    "    features = np.array(features).reshape(1, -1)  # Reshape for single sample\n",
    "    \n",
    "    # 标准化特征\n",
    "    scaler = StandardScaler()\n",
    "    features = scaler.fit_transform(features)\n",
    "    \n",
    "    # 进行预测\n",
    "    prediction = model.predict(features)\n",
    "    \n",
    "    # 输出预测结果\n",
    "    print(f\"Prediction: {prediction[0]}\")\n",
    "\n",
    "# 运行主函数\n",
    "main()\n"
   ],
   "id": "8b524112b9f99af1",
   "execution_count": 32,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T06:14:09.198564Z",
     "start_time": "2024-07-08T06:14:08.967973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "def extract_features_from_window(window):\n",
    "    features = []\n",
    "    for column in window.columns:\n",
    "        sensor_data = window[column]\n",
    "        \n",
    "        # Detect positive pulses\n",
    "        pulses = sensor_data[sensor_data > sensor_data.mean()]\n",
    "        \n",
    "        # Extract features: height, duration, area, max, min, mean, std\n",
    "        heights = pulses.values\n",
    "        durations = np.diff(pulses.index)\n",
    "        \n",
    "        # Adjust lengths to match\n",
    "        if len(heights) > 1:\n",
    "            heights = heights[:-1]  # Drop the last height to match durations length\n",
    "        \n",
    "        areas = heights * durations if len(heights) > 0 else np.array([0])\n",
    "        max_height = np.max(heights) if len(heights) > 0 else 0\n",
    "        min_height = np.min(heights) if len(heights) > 0 else 0\n",
    "        mean_height = np.mean(heights) if len(heights) > 0 else 0\n",
    "        std_height = np.std(heights) if len(heights) > 0 else 0\n",
    "        pulse_frequency = len(heights) / len(sensor_data)\n",
    "        \n",
    "        # Additional features\n",
    "        energy = np.sum(sensor_data ** 2)  # Signal energy\n",
    "        mean = np.mean(sensor_data)  # Mean value\n",
    "        std = np.std(sensor_data)  # Standard deviation\n",
    "        var = np.var(sensor_data)  # Variance\n",
    "        rms = np.sqrt(np.mean(sensor_data ** 2))  # Root mean square\n",
    "        skewness = pd.Series(sensor_data).skew()  # Skewness\n",
    "        kurtosis = pd.Series(sensor_data).kurt()  # Kurtosis\n",
    "        \n",
    "        features.extend([\n",
    "            max_height, min_height, mean_height, std_height, pulse_frequency,\n",
    "            energy, mean, std, var, rms, skewness, kurtosis\n",
    "        ])\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_features_from_data(df, window_size=89):\n",
    "    X = []\n",
    "    for start in range(0, len(df) - window_size + 1, window_size):\n",
    "        window = df.iloc[start:start + window_size]\n",
    "        features = extract_features_from_window(window)\n",
    "        X.append(features)\n",
    "    return np.array(X)\n",
    "\n",
    "def main():\n",
    "    # 加载模型\n",
    "    model = joblib.load('best_svm_model.pkl')\n",
    "    print(\"Model loaded from 'best_svm_model.pkl'\")\n",
    "    \n",
    "    # 加载数据文件\n",
    "    file_path = 'left_processed.csv'\n",
    "    df = load_data(file_path)\n",
    "    \n",
    "    # 提取特征\n",
    "    features = extract_features_from_data(df)\n",
    "    \n",
    "    # 标准化特征\n",
    "    scaler = StandardScaler()\n",
    "    features = scaler.fit_transform(features)\n",
    "    \n",
    "    # 进行预测\n",
    "    predictions = model.predict(features)\n",
    "    \n",
    "    # 统计每种状态的预测次数\n",
    "    unique, counts = np.unique(predictions, return_counts=True)\n",
    "    prediction_counts = dict(zip(unique, counts))\n",
    "    \n",
    "    # 输出预测结果统计\n",
    "    print(\"Prediction counts:\")\n",
    "    for state, count in prediction_counts.items():\n",
    "        print(f\"{state}: {count}\")\n",
    "    \n",
    "    # 打印分类报告\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(predictions, predictions, zero_division=0))\n",
    "\n",
    "# 运行主函数\n",
    "main()\n"
   ],
   "id": "d132d75d237f26e6",
   "execution_count": 68,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-08T06:18:14.305766Z",
     "start_time": "2024-07-08T06:18:13.438809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import glob\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "def extract_features_from_window(window):\n",
    "    features = []\n",
    "    for column in window.columns:\n",
    "        sensor_data = window[column]\n",
    "        \n",
    "        # Detect positive pulses\n",
    "        pulses = sensor_data[sensor_data > sensor_data.mean()]\n",
    "        \n",
    "        # Extract features: height, duration, area, max, min, mean, std\n",
    "        heights = pulses.values\n",
    "        durations = np.diff(pulses.index)\n",
    "        \n",
    "        # Adjust lengths to match\n",
    "        if len(heights) > 1:\n",
    "            heights = heights[:-1]  # Drop the last height to match durations length\n",
    "        \n",
    "        areas = heights * durations if len(heights) > 0 else np.array([0])\n",
    "        max_height = np.max(heights) if len(heights) > 0 else 0\n",
    "        min_height = np.min(heights) if len(heights) > 0 else 0\n",
    "        mean_height = np.mean(heights) if len(heights) > 0 else 0\n",
    "        std_height = np.std(heights) if len(heights) > 0 else 0\n",
    "        pulse_frequency = len(heights) / len(sensor_data)\n",
    "        \n",
    "        # Additional features\n",
    "        energy = np.sum(sensor_data ** 2)  # Signal energy\n",
    "        mean = np.mean(sensor_data)  # Mean value\n",
    "        std = np.std(sensor_data)  # Standard deviation\n",
    "        var = np.var(sensor_data)  # Variance\n",
    "        rms = np.sqrt(np.mean(sensor_data ** 2))  # Root mean square\n",
    "        skewness = pd.Series(sensor_data).skew()  # Skewness\n",
    "        kurtosis = pd.Series(sensor_data).kurt()  # Kurtosis\n",
    "        \n",
    "        features.extend([\n",
    "            max_height, min_height, mean_height, std_height, pulse_frequency,\n",
    "            energy, mean, std, var, rms, skewness, kurtosis\n",
    "        ])\n",
    "    \n",
    "    return features\n",
    "\n",
    "def extract_features_from_data(df, window_size=89):\n",
    "    X = []\n",
    "    for start in range(0, len(df) - window_size + 1, window_size):\n",
    "        window = df.iloc[start:start + window_size]\n",
    "        features = extract_features_from_window(window)\n",
    "        X.append(features)\n",
    "    return np.array(X)\n",
    "\n",
    "def main():\n",
    "    # 加载模型\n",
    "    rf_model = joblib.load('best_random_forest_model.pkl')\n",
    "    svm_model = joblib.load('best_svm_model.pkl')\n",
    "    print(\"Models loaded from 'best_random_forest_model.pkl' and 'best_svm_model.pkl'\")\n",
    "    \n",
    "    # 获取所有数据文件路径\n",
    "    file_paths = glob.glob('*_processed.csv')\n",
    "    \n",
    "    all_predictions_rf = []\n",
    "    all_predictions_svm = []\n",
    "    all_labels = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        # 从文件名中提取状态\n",
    "        state = file_path.split('/')[-1].split('_')[0]\n",
    "        \n",
    "        # 加载数据文件\n",
    "        df = load_data(file_path)\n",
    "        \n",
    "        # 提取特征\n",
    "        features = extract_features_from_data(df)\n",
    "        \n",
    "        # 标准化特征\n",
    "        scaler = StandardScaler()\n",
    "        features = scaler.fit_transform(features)\n",
    "        \n",
    "        # 使用随机森林模型进行预测\n",
    "        predictions_rf = rf_model.predict(features)\n",
    "        all_predictions_rf.extend(predictions_rf)\n",
    "        \n",
    "        # 使用SVM模型进行预测\n",
    "        predictions_svm = svm_model.predict(features)\n",
    "        all_predictions_svm.extend(predictions_svm)\n",
    "        \n",
    "        # 添加真实标签\n",
    "        all_labels.extend([state] * len(predictions_rf))\n",
    "    \n",
    "    # 统计每种状态的预测次数（随机森林）\n",
    "    unique_rf, counts_rf = np.unique(all_predictions_rf, return_counts=True)\n",
    "    prediction_counts_rf = dict(zip(unique_rf, counts_rf))\n",
    "    \n",
    "    # 统计每种状态的预测次数（SVM）\n",
    "    unique_svm, counts_svm = np.unique(all_predictions_svm, return_counts=True)\n",
    "    prediction_counts_svm = dict(zip(unique_svm, counts_svm))\n",
    "    \n",
    "    # 输出预测结果统计\n",
    "    print(\"Random Forest Prediction counts:\")\n",
    "    for state, count in prediction_counts_rf.items():\n",
    "        print(f\"{state}: {count}\")\n",
    "    \n",
    "    print(\"SVM Prediction counts:\")\n",
    "    for state, count in prediction_counts_svm.items():\n",
    "        print(f\"{state}: {count}\")\n",
    "    \n",
    "    # 打印分类报告\n",
    "    print(\"Random Forest Classification Report:\")\n",
    "    print(classification_report(all_labels, all_predictions_rf, zero_division=0))\n",
    "    \n",
    "    print(\"SVM Classification Report:\")\n",
    "    print(classification_report(all_labels, all_predictions_svm, zero_division=0))\n",
    "\n",
    "# 运行主函数\n",
    "main()\n"
   ],
   "id": "f8ef0fc333f19a27",
   "execution_count": 69,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## CNN classification",
   "id": "a4235e4810bf2139"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "def extract_features_from_data(df, window_size=89):\n",
    "    X = []\n",
    "    for start in range(0, len(df) - window_size + 1, window_size):\n",
    "        window = df.iloc[start:start + window_size].values\n",
    "        X.append(window)\n",
    "    return np.array(X)\n",
    "\n",
    "def load_and_process_files(file_paths, window_size=89):\n",
    "    all_X = []\n",
    "    all_y = []\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        # Extract state from the file name\n",
    "        state = file_path.split('/')[-1].split('_')[0]\n",
    "        \n",
    "        # Load and process the CSV file\n",
    "        df = load_data(file_path)\n",
    "        \n",
    "        # Prepare dataset from windows\n",
    "        X = extract_features_from_data(df, window_size)\n",
    "        y = [state] * len(X)\n",
    "        \n",
    "        all_X.append(X)\n",
    "        all_y.extend(y)\n",
    "    \n",
    "    # Combine data from all files\n",
    "    all_X = np.vstack(all_X)\n",
    "    all_y = np.array(all_y)\n",
    "    \n",
    "    return all_X, all_y\n",
    "\n",
    "# 将状态标签转换为数值编码\n",
    "def encode_labels(labels):\n",
    "    unique_labels = np.unique(labels)\n",
    "    label_to_index = {label: index for index, label in enumerate(unique_labels)}\n",
    "    return np.array([label_to_index[label] for label in labels]), label_to_index\n",
    "\n",
    "def main():\n",
    "    # 数据文件路径\n",
    "    file_paths = glob.glob('*_processed.csv')  # Adjust the path as necessary\n",
    "    window_size = 89  # Set the window size to 89 or 90 based on your preference\n",
    "    \n",
    "    # 加载和处理文件\n",
    "    all_X, all_y = load_and_process_files(file_paths, window_size)\n",
    "    \n",
    "    # 将状态标签转换为数值编码\n",
    "    all_y, label_to_index = encode_labels(all_y)\n",
    "    all_y = to_categorical(all_y)\n",
    "    \n",
    "    # 标准化特征\n",
    "    scaler = StandardScaler()\n",
    "    all_X = scaler.fit_transform(all_X.reshape(-1, window_size * 4)).reshape(-1, window_size, 4)\n",
    "    \n",
    "    # 划分训练集和测试集\n",
    "    X_train, X_test, y_train, y_test = train_test_split(all_X, all_y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # 创建CNN模型\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(window_size, 4)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Conv1D(128, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(len(label_to_index), activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    # 训练模型\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=50, batch_size=32, callbacks=[early_stopping])\n",
    "    \n",
    "    # 评估模型\n",
    "    loss, accuracy = model.evaluate(X_test, y_test)\n",
    "    print(f\"Test Accuracy: {accuracy}\")\n",
    "    \n",
    "    # 保存模型\n",
    "    model.save('cnn_model.h5')\n",
    "    print(\"Model saved as 'cnn_model.h5'\")\n",
    "    \n",
    "    # 打印分类报告\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_labels = np.argmax(y_pred, axis=1)\n",
    "    y_test_labels = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test_labels, y_pred_labels, target_names=label_to_index.keys()))\n",
    "\n",
    "# 运行主函数\n",
    "main()\n"
   ],
   "id": "11dd370ed798b501",
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
